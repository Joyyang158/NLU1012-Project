{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "V4Z4EfeIGFXW",
        "K9tzVe-nH7Yn",
        "D39th6R7HrCd",
        "LhmjSSsRIQhT",
        "_PrO84ITIbro",
        "PJBbMJYsuceO",
        "rmRcbq0F5jbS",
        "ez2hQt7j53ul",
        "fREhVX9V5Wzt"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Packages and Initial Setup"
      ],
      "metadata": {
        "id": "Nhi64gpoFLvP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install openai datasets together"
      ],
      "metadata": {
        "id": "1lkcPQKIFOtQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import requests\n",
        "import random\n",
        "import time\n",
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "import together\n",
        "from openai import OpenAI\n",
        "import os\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "\n",
        "TOGETHER_API_KEY = # this key stems from togetherAI.\n",
        "GPT_API_KEY = # this key stems from openAI."
      ],
      "metadata": {
        "id": "r5rPJTy0Fk7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_name = \"meta-llama/Llama-3-70b-chat-hf\"\n",
        "# model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "# model_name = \"lmsys/vicuna-7b-v1.5\"\n",
        "# model_name = \"Qwen/Qwen1.5-7B-Chat\"\n",
        "# model_name = \"QWEN/QWEN1.5-72B-CHAT\"\n",
        "# model_name = \"gpt-3.5-turbo-0125\""
      ],
      "metadata": {
        "id": "StMMY1euEo8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Datasets - TATQA, WikiTQ and HybridQA"
      ],
      "metadata": {
        "id": "V4Z4EfeIGFXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TATQA\n",
        "random.seed(42)\n",
        "file_path = ''#TAT-QA-train-markdown.json\n",
        "with open(file_path, 'r') as file:\n",
        "    dataset_TAT = json.load(file)\n",
        "test = random.sample(dataset_TAT, 500)\n",
        "\n",
        "def extract_questions(text_list):\n",
        "    questions = []\n",
        "    for text in text_list:\n",
        "        # Find the start of the question part\n",
        "        question_index = text['input'].find('\\nQuestion:\\n')\n",
        "        if question_index != -1:\n",
        "            # Extract everything after 'Question:\\n'\n",
        "            question = text['input'][question_index:]\n",
        "            questions.append(question)\n",
        "    return questions\n",
        "\n",
        "def extract_tables(x):\n",
        "  tables = []\n",
        "  start_marker = \"Table:\\n\"\n",
        "  end_marker = \"\\nQuestion:\\n\"\n",
        "  for item_input in x:\n",
        "    end_index = item_input['input'].find(end_marker)\n",
        "    tables.append(item_input['input'][len(start_marker)+1:end_index])\n",
        "  return tables\n",
        "\n",
        "questions = extract_questions(test)\n",
        "tables = extract_tables(test)\n",
        "test = pd.DataFrame(test)\n",
        "test['Question'] = questions\n",
        "test['Question'] = test['Question'].str.replace(\"\\nQuestion:\\n\", \"\", regex=False)\n",
        "test['Table']  = tables\n",
        "test = test[['Question','Table','output']]\n",
        "test.rename(columns={'output': 'True'}, inplace=True)"
      ],
      "metadata": {
        "id": "qOupL-xaGHy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# WikiTQ\n",
        "dataset = load_dataset(\"wikitablequestions\", split = 'test', trust_remote_code=True)\n",
        "random_seed = 42\n",
        "df = pd.DataFrame(dataset)\n",
        "test = df.sample(n = 500, random_state = random_seed)\n",
        "test = test.reset_index(drop=True)\n",
        "test.rename(columns={'question': 'Question', 'answers': 'True', 'table': 'Table'}, inplace=True)\n",
        "test.head()"
      ],
      "metadata": {
        "id": "et05VjUeG4YE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HybridQA\n",
        "test = pd.read_csv('') # hybrid_4096.csv\n",
        "test.rename(columns={'question': 'Question', 'table': 'Table', 'answer_text': 'True'}, inplace=True)"
      ],
      "metadata": {
        "id": "oDjl8XpnG7_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT4 Evaluator"
      ],
      "metadata": {
        "id": "K9tzVe-nH7Yn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_w_gpt4(f):\n",
        "  client = OpenAI(api_key = GPT_API_KEY)\n",
        "  system = '''You are an intelligent assessment assistant.'''\n",
        "  prefix = '''Based on the question and the golden answer, judge whether the predicted answer correctly answers the question and give only a Yes or No.\\n'''\n",
        "\n",
        "  df = pd.read_csv(f)\n",
        "\n",
        "  for i in range(len(df)):\n",
        "    question = df.loc[i, \"Question\"]\n",
        "    answer = df.loc[i, \"True\"]\n",
        "    prediction = df.loc[i, \"Prediction\"]\n",
        "\n",
        "    prompt = prefix + f\"Question: [{question}]\\nGolden answer: [{answer}]\\nPredicted answer: [{prediction}]\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4\",\n",
        "        messages=[\n",
        "          {\"role\": \"system\", \"content\": system},\n",
        "          {\"role\": \"user\", \"content\": prompt},\n",
        "        ]\n",
        "      )\n",
        "    output = response.choices[0].message.content\n",
        "    df.loc[i, \"GPT4_result\"] = output\n",
        "    print(i, output)\n",
        "\n",
        "  num_of_yes = df['GPT4_result'].value_counts()['Yes']\n",
        "  acc = (num_of_yes / len(df))\n",
        "  print(\"Accuracy:\", acc)\n",
        "  df.to_csv(f, index=False)"
      ],
      "metadata": {
        "id": "jQHyo2xNIADI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline Vanilla"
      ],
      "metadata": {
        "id": "D39th6R7HrCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = # model name\n",
        "file_name = # saved file path\n",
        "start = 0\n",
        "max = len(test)\n",
        "end = max"
      ],
      "metadata": {
        "id": "cxxOn13HHwua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in tqdm(range(start, end)):\n",
        "  question = test.loc[i, \"Question\"]\n",
        "  table = test.loc[i, \"Table\"]\n",
        "  answer = test.loc[i, \"True\"]\n",
        "\n",
        "  client = OpenAI(\n",
        "    api_key=TOGETHER_API_KEY,\n",
        "    base_url='https://api.together.xyz/v1',\n",
        "  )\n",
        "  prompt = \"{} Given this table, answer this question {}. You do not need to explain the answer.\".format(table, question)\n",
        "\n",
        "  try:\n",
        "    chat_completion = client.chat.completions.create(\n",
        "      messages=[\n",
        "        {\n",
        "          \"role\": \"system\",\n",
        "          \"content\": \"You handle table information well.\",\n",
        "        },\n",
        "        {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": f'''<bos><start_of_turn>user\\n{prompt}<end_of_turn>\\n<start_of_turn>model''' if model_name == \"google/gemma-7b-it\" else prompt,\n",
        "        }\n",
        "      ],\n",
        "      model=model_name\n",
        "    )\n",
        "\n",
        "    prediction = chat_completion.choices[0].message.content\n",
        "    with open(file_name, 'a+', newline='') as file:\n",
        "      writer = csv.writer(file)\n",
        "      if os.path.getsize(file_name) == 0:\n",
        "          writer.writerow([\"Question\", \"Table\", \"Prediction\", \"True\"])\n",
        "      writer.writerow([question, table, prediction, answer])\n",
        "  except:\n",
        "    print(\"Error\")\n",
        "    continue"
      ],
      "metadata": {
        "id": "dV9JLIBdH0CL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_w_gpt4(file_name)"
      ],
      "metadata": {
        "id": "2JZNaQLgIJDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baseline Self-Augmentation"
      ],
      "metadata": {
        "id": "LhmjSSsRIQhT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = # model name\n",
        "file_name = # saved file path\n",
        "start = 0\n",
        "max = len(test)\n",
        "end = 100"
      ],
      "metadata": {
        "id": "UxCaC2bmIUTn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in tqdm(range(start, end)):\n",
        "  question = test.loc[i, \"Question\"]\n",
        "  table = test.loc[i, \"Table\"]\n",
        "  answer = test.loc[i, \"True\"]\n",
        "\n",
        "  client = OpenAI(\n",
        "    api_key=TOGETHER_API_KEY,\n",
        "    base_url='https://api.together.xyz/v1',\n",
        "  )\n",
        "\n",
        "  # First Phase: Generating Intermediate Structural Knowledge\n",
        "  prompt_phase_1 = f'''{table} Analyze this table and identify the critical values, ranges, and necessary calculations needed to answer the question below:\n",
        "  Question: {question}'''\n",
        "\n",
        "  try:\n",
        "    chat_completion_1 = client.chat.completions.create(\n",
        "      messages=[\n",
        "        {\n",
        "          \"role\": \"system\",\n",
        "          \"content\": \"You handle table information well.\",\n",
        "        },\n",
        "        {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": f'''<bos><start_of_turn>user\\n{prompt_phase_1}<end_of_turn>\\n<start_of_turn>model''' if model_name == \"google/gemma-7b-it\" else prompt_phase_1,\n",
        "        }\n",
        "      ],\n",
        "      model=model_name\n",
        "    )\n",
        "\n",
        "    intermediate = chat_completion_1.choices[0].message.content\n",
        "\n",
        "    # Second Phase: Using the guidance to answer the question\n",
        "    prompt_phase_2 = f'''{table} Given this table, answer this question: {question}. Additional information of the table: {intermediate}'''\n",
        "\n",
        "    chat_completion_2 = client.chat.completions.create(\n",
        "      messages=[\n",
        "        {\n",
        "          \"role\": \"system\",\n",
        "          \"content\": \"You handle table information well.\",\n",
        "        },\n",
        "        {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": f'''<bos><start_of_turn>user\\n{prompt_phase_2}<end_of_turn>\\n<start_of_turn>model''' if model_name == \"google/gemma-7b-it\" else prompt_phase_2,\n",
        "        }\n",
        "      ],\n",
        "      model=model_name\n",
        "    )\n",
        "\n",
        "    prediction = chat_completion_2.choices[0].message.content\n",
        "\n",
        "    with open(file_name, 'a+', newline='') as file:\n",
        "      writer = csv.writer(file)\n",
        "      if os.path.getsize(file_name) == 0:\n",
        "          writer.writerow([\"Question\", \"Table\", \"Prediction\", \"True\"])\n",
        "      writer.writerow([question, table, prediction, answer])\n",
        "  except:\n",
        "    print(\"Error\")\n",
        "    continue"
      ],
      "metadata": {
        "id": "E0LvkKcMIXnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_w_gpt4(file_name)"
      ],
      "metadata": {
        "id": "tTN3P1LFIYJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Table-Logic Sequential Prompting"
      ],
      "metadata": {
        "id": "_PrO84ITIbro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "step1 = ''' Table: {table}\\nIdentify critical columns highly relevant to this question: {question}. Be brief and short. Do NOT include answer.'''\n",
        "step2 = ''' Table: {table}\\nCritical columns: {column}\\nBased on the critical columns of this table, identify critical rows highly relevant to this question: {question}. Be brief and short. Do NOT include answer.'''\n",
        "step3 = ''' Table: {table}\\nCritical columns: {column}\\nCritical rows: {row}\\nBased on the key values in critical columns and rows of this table, identify any aggregation, calculation, and comparison required by this question: {question}. Be brief and short. Do NOT include answer.'''\n",
        "step4 = ''' Table: {table}\\nGiven this table, answer this question: {question}. You do not need to explain the answer. Additional information that may help is given below. Critical columns: {column}\\nCritical rows: {row}\\nAggregation, calculation, and comparison: {aggregation}'''"
      ],
      "metadata": {
        "id": "tin1q8rbJHo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(prompt, model_name):\n",
        "  chat_completion = client.chat.completions.create(\n",
        "  messages=[\n",
        "    {\n",
        "      \"role\": \"system\",\n",
        "      \"content\": \"You handle table information well.\",\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": prompt,\n",
        "    }\n",
        "    ],\n",
        "  model=model_name,\n",
        "  max_tokens = 4096 # 500 for vicuna-7b and llama2-7b\n",
        "  )\n",
        "\n",
        "  result = chat_completion.choices[0].message.content\n",
        "  return result"
      ],
      "metadata": {
        "id": "pdEeIwrmKzXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import csv\n",
        "import os\n",
        "file_name = \"\" # saved file path\n",
        "model_name = \"\" # model name\n",
        "for i in tqdm(range(len(df))):\n",
        "  question = df.loc[i, \"Question\"]\n",
        "  table = df.loc[i, \"Table\"]\n",
        "  answer = df.loc[i, \"True\"]\n",
        "\n",
        "  client = OpenAI(\n",
        "      api_key=TOGETHER_API_KEY,\n",
        "      base_url='https://api.together.xyz/v1',\n",
        "  )\n",
        "\n",
        "  # For GPT3.5 model:\n",
        "  # client = OpenAI(\n",
        "  #   api_key=OPEN_AI_API_KEY\n",
        "  # )\n",
        "\n",
        "  try:\n",
        "    # ************ Step1 ************\n",
        "    prompt1 = step1.format(table = table, question = question)\n",
        "    column = inference(prompt1, model_name)\n",
        "\n",
        "    # ************ Step2 ************\n",
        "    prompt2 = step2.format(table = table, question = question, column = column)\n",
        "    row = inference(prompt2, model_name)\n",
        "\n",
        "    # ************ Step3 ************\n",
        "    prompt3 = step3.format(table = table, question = question, column = column, row = row)\n",
        "    aggregation = inference(prompt3, model_name)\n",
        "\n",
        "    # ************ Step4 ************\n",
        "    prompt4 = step4.format(table = table, question = question, column = column, row = row, aggregation = aggregation)\n",
        "    prediction = inference(prompt4, model_name)\n",
        "\n",
        "    with open(file_name, 'a+', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        if os.path.getsize(file_name) == 0:\n",
        "            writer.writerow([\"Question\", \"Table\", \"True\", \"Column\", \"Row\", \"Aggregation\", \"Prediction\"])\n",
        "        writer.writerow([question, table, answer, column, row, aggregation, prediction])\n",
        "  except Exception as E:\n",
        "    print(E)\n",
        "    df.drop(index=i)"
      ],
      "metadata": {
        "id": "gvXXOZgqK3N0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Table-Logic Segment Experiment"
      ],
      "metadata": {
        "id": "PJBbMJYsuceO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "step1 = ''' Table: {table}\\nGiven this table, ONLY focus on information of table in columns: {column} and rows: {row}\\n Answer this question: {question}. You do not need to explain the answer. '''\n",
        "step2 = ''' Table: {table}\\nGiven this table, answer this question {question} STRICTLY follow the instruction of Aggregation, calculation, and comparison needed: {aggregation}. You do not need to explain the answer. '''"
      ],
      "metadata": {
        "id": "lHZxF91Au7EW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def inference(prompt, model_name):\n",
        "  chat_completion = client.chat.completions.create(\n",
        "  messages=[\n",
        "    {\n",
        "      \"role\": \"system\",\n",
        "      \"content\": \"You handle table information well.\",\n",
        "    },\n",
        "    {\n",
        "      \"role\": \"user\",\n",
        "      \"content\": prompt,\n",
        "    }\n",
        "    ],\n",
        "  model=model_name,\n",
        "  max_tokens = 4096 # 500 for vicuna-7b and llama2-7b\n",
        "  )\n",
        "  result = chat_completion.choices[0].message.content\n",
        "  return result"
      ],
      "metadata": {
        "id": "6pfuRohPuf0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = # model name\n",
        "file_name = # saved file path"
      ],
      "metadata": {
        "id": "afKk9k-Turyb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('') # load generated instruction dataset from other models"
      ],
      "metadata": {
        "id": "pqqqCdkZ0O_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import csv\n",
        "import os\n",
        "for i in tqdm(range(len(df))):\n",
        "  question = df.loc[i, \"Question\"]\n",
        "  table = df.loc[i, \"Table\"]\n",
        "  column = df.loc[i, \"Column\"]\n",
        "  row = df.loc[i, \"Row\"]\n",
        "  aggregation = df.loc[i,\"Aggregation\"]\n",
        "  answer = df.loc[i, \"True\"]\n",
        "\n",
        "  client = OpenAI(\n",
        "      api_key=TOGETHER_API_KEY,\n",
        "      # base_url='https://api.together.xyz/v1'if using togetherAI,\n",
        "  )\n",
        "  try:\n",
        "\n",
        "    # Given critical columns and rows generated from other models\n",
        "    prompt = step1.format(table = table, column = column, row = row, question = question)\n",
        "    prediction = inference(prompt, model_name)\n",
        "\n",
        "    # Given aggregation generated from other models\n",
        "    # prompt = step2.format(table = table, question = question, aggregation = aggregation)\n",
        "    # prediction = inference(prompt, model_name)\n",
        "\n",
        "    # Given table instruction\n",
        "    # TATQA instruction\n",
        "    instruction = \"Please read the following table in Markdown format and related paragraphs, and then answer the question according to the table and paragraphs. Table cells in one row are seperated by '|', and different rows are seperated by '\\n'.\"\n",
        "\n",
        "    # WikiTQ instruction\n",
        "    # instruction = \"Data is structured in a dictionary with keys 'header' for column names and 'rows' for data, formatted as lists under each key.\"\n",
        "\n",
        "    # HybridQA instruction\n",
        "    # instruction = \"Data is organized in a dictionary with keys for URL, title, headers, and rows. Headers define column titles, and rows are lists containing cell data and optional URLs for detailed information.\"\n",
        "\n",
        "    prompt = \"{} {} Given this table, answer this question {}. You do not need to explain the answer.\".format(instructuion, table, question)\n",
        "\n",
        "    with open(file_name, 'a+', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        if os.path.getsize(file_name) == 0:\n",
        "            writer.writerow([\"Question\", \"Table\", \"True\", \"Prediction\"])\n",
        "        writer.writerow([question, table, answer, prediction])\n",
        "  except Exception as E:\n",
        "    print(E)\n",
        "    df.drop(index=i)"
      ],
      "metadata": {
        "id": "RmMnfe24uuKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_w_gpt4(file_name)"
      ],
      "metadata": {
        "id": "v5KBkHuvu1z6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Value Location Experiment"
      ],
      "metadata": {
        "id": "rmRcbq0F5jbS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Col Row Retrieve"
      ],
      "metadata": {
        "id": "toExJ4u55pN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def find_tuple(string):\n",
        "  pattern = r\"\\((-?\\d+),\\s*(-?\\d+)\\)\"\n",
        "  match = re.search(pattern, string)\n",
        "  if match:\n",
        "      tuple_result = (int(match.group(1)), int(match.group(2)))\n",
        "      return tuple_result\n",
        "  else:\n",
        "      print(\"No tuple found.\")"
      ],
      "metadata": {
        "id": "Kzvmtbzb5nx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = # model name\n",
        "start = 0\n",
        "max = len(df)\n",
        "end = max\n",
        "\n",
        "file_name = '/content/drive/MyDrive/12-final_project/value_loc_gpt3.5_result.csv'"
      ],
      "metadata": {
        "id": "BcfVBGTs5w9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in tqdm(range(start, end)):\n",
        "  # question = test.loc[i, \"Question\"]\n",
        "  table = df.loc[i, \"Table\"]\n",
        "  # answer = test.loc[i, \"True\"]\n",
        "  value = df.loc[i, \"Value\"]\n",
        "  row = df.loc[i, \"Row\"]\n",
        "  column = df.loc[i, \"Column\"]\n",
        "\n",
        "  client = OpenAI(\n",
        "    api_key=TOGETHER_API_KEY,\n",
        "    base_url='https://api.together.xyz/v1',\n",
        "  )\n",
        "  prompt = '''{} \\nGiven this table, provide the position of {} as a tuple, where the first number represents row index and the second number represents column index.\n",
        "  You do not need to explain the answer.'''.format(table, value)\n",
        "  system_message = \"You handle table information well.\"\n",
        "\n",
        "  try:\n",
        "    chat_completion = client.chat.completions.create(\n",
        "      messages=[\n",
        "        {\n",
        "          \"role\": \"system\",\n",
        "          \"content\": \"You handle table information well.\",\n",
        "        },\n",
        "        {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": prompt,\n",
        "        }\n",
        "      ],\n",
        "      model=model_name,\n",
        "      max_tokens=2048,\n",
        "    )\n",
        "\n",
        "    pred = chat_completion.choices[0].message.content\n",
        "    rc = find_tuple(pred)\n",
        "    with open(file_name, 'a+', newline='') as file:\n",
        "      writer = csv.writer(file)\n",
        "      if os.path.getsize(file_name) == 0:\n",
        "          writer.writerow([\"Table\", \"Value\", \"Row\", \"Column\", \"pred_row\", \"pred_column\"])\n",
        "      writer.writerow([table, value, row, column, rc[0], rc[1]])\n",
        "  except Exception as E:\n",
        "    print(E)\n",
        "    continue"
      ],
      "metadata": {
        "id": "5xIOXlaS5xw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Value Look-up"
      ],
      "metadata": {
        "id": "ez2hQt7j53ul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_value_from_brackets(text):\n",
        "    start = text.find('[') + 1\n",
        "    end = text.find(']', start)\n",
        "    if start > 0 and end > 0:\n",
        "        return text[start:end]\n",
        "    else:\n",
        "        return \"Value not found\""
      ],
      "metadata": {
        "id": "hpLBC69L59J_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = # model name\n",
        "\n",
        "start = 0\n",
        "max = len(df)\n",
        "end = max\n",
        "\n",
        "file_name = # saved file path"
      ],
      "metadata": {
        "id": "wwPjLYwm59uo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in tqdm(range(start, end)):\n",
        "  # question = test.loc[i, \"Question\"]\n",
        "  table = df.loc[i, \"Table\"]\n",
        "  # answer = test.loc[i, \"True\"]\n",
        "  value = df.loc[i, \"Value\"]\n",
        "  row = df.loc[i, \"Row\"]\n",
        "  column = df.loc[i, \"Column\"]\n",
        "  pred_row = df.loc[i, \"pred_row\"]\n",
        "  pred_column = df.loc[i, \"pred_column\"]\n",
        "\n",
        "  client = OpenAI(\n",
        "    # api_key=TOGETHER_API_KEY,\n",
        "    # base_url='https://api.together.xyz/v1',\n",
        "    api_key=GPT_API_KEY,\n",
        "  )\n",
        "\n",
        "  prompt = f'''{table} \\nGiven this table, provide the cell value at row index {row} and column index {column} in square brackets. Example: [cell value]\n",
        "    You do not need to explain the answer.'''\n",
        "  system_message = \"You handle table information well.\"\n",
        "\n",
        "  try:\n",
        "    chat_completion = client.chat.completions.create(\n",
        "      messages=[\n",
        "        {\n",
        "          \"role\": \"system\",\n",
        "          \"content\": \"You handle table information well.\",\n",
        "        },\n",
        "        {\n",
        "          \"role\": \"user\",\n",
        "          \"content\": prompt,\n",
        "        }\n",
        "      ],\n",
        "      model=model_name,\n",
        "      max_tokens=2048,\n",
        "    )\n",
        "\n",
        "    pred = chat_completion.choices[0].message.content\n",
        "    pred_val = extract_value_from_brackets(pred)\n",
        "\n",
        "    with open(file_name, 'a+', newline='') as file:\n",
        "      writer = csv.writer(file)\n",
        "      if os.path.getsize(file_name) == 0:\n",
        "          writer.writerow([\"Table\", \"Value\", \"Row\", \"Column\", \"pred_row\", \"pred_col\", \"pred_val\"])\n",
        "      writer.writerow([table, value, row, column, pred_row, pred_column, pred_val])\n",
        "  except Exception as E:\n",
        "    print(E)\n",
        "    continue"
      ],
      "metadata": {
        "id": "kz27mhU35_Zv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Plot Radar Chart"
      ],
      "metadata": {
        "id": "fREhVX9V5Wzt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from math import pi\n",
        "\n",
        "# Data\n",
        "data = {\n",
        "    'Model': ['Llama3-70B', 'Llama2-7B', 'Vicuna-7B', 'QWen-7B'],\n",
        "    'Table Partition_Row': [0.44, 0.21, 0.08, 0.23],\n",
        "    'Table Partition_Column': [0.94, 0.36, 0.21, 0.45],\n",
        "    'Value Lookup': [0.40, 0.08, 0.05, 0.12],\n",
        "    'Column Finding': [0.79, 0.11, 0.20, 0.31],\n",
        "    'Row Finding': [0.52, 0.15, 0.14, 0.18]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Number of variables\n",
        "categories = list(df)[1:]\n",
        "N = len(categories)\n",
        "\n",
        "# What will be the angle of each axis in the plot?\n",
        "angles = [n / float(N) * 2 * pi for n in range(N)]\n",
        "angles += angles[:1]\n",
        "\n",
        "# Initialise the radar plot\n",
        "ax = plt.subplot(111, polar=True)\n",
        "\n",
        "# Draw one axe per variable + add labels\n",
        "plt.xticks(angles[:-1], categories)\n",
        "\n",
        "# Draw ylabels\n",
        "ax.set_rlabel_position(0)\n",
        "plt.yticks([0.1,0.3,0.5,0.7,0.9], [\"0.1\",\"0.3\",\"0.5\",\"0.7\",\"0.9\"], color=\"grey\", size=7)\n",
        "plt.ylim(0,1)\n",
        "\n",
        "# Plot each model\n",
        "for index, row in df.iterrows():\n",
        "    values = row.drop('Model').values.flatten().tolist()\n",
        "    values += values[:1]\n",
        "    ax.plot(angles, values, linewidth=1, linestyle='solid', label=row['Model'])\n",
        "    ax.fill(angles, values, alpha=0.1)\n",
        "\n",
        "# Add a legend\n",
        "plt.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GMUaNhw15bOt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}